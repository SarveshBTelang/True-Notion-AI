{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f907c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Notion API keys\n",
    "NOTION_TOKEN = os.getenv('NOTION_TOKEN')\n",
    "DATABASE_ID = os.getenv('DATABASE_ID')\n",
    "\n",
    "if NOTION_TOKEN is None:\n",
    "    raise ValueError(\"NOTION_API_KEY not found in .env file\")\n",
    "\n",
    "if DATABASE_ID is None:\n",
    "    raise ValueError(\"DATABASE_ID not found in .env file\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + NOTION_TOKEN,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": \"2022-06-28\",\n",
    "}\n",
    "\n",
    "def extract_notion_rows(notion_response):\n",
    "    results = []\n",
    "    \n",
    "    for page in notion_response.get(\"results\", []):\n",
    "        page_data = {\n",
    "            \"id\": page.get(\"id\"),\n",
    "            \"properties\": {}\n",
    "        }\n",
    "        properties = page.get(\"properties\", {})\n",
    "        \n",
    "        for prop_name, prop_value in properties.items():\n",
    "            prop_type = prop_value.get(\"type\")\n",
    "\n",
    "            if prop_type == \"rich_text\":\n",
    "                value = \"\".join([rt.get(\"plain_text\", \"\") for rt in prop_value.get(\"rich_text\", [])])\n",
    "            elif prop_type == \"title\":\n",
    "                value = \"\".join([t.get(\"plain_text\", \"\") for t in prop_value.get(\"title\", [])])\n",
    "            elif prop_type == \"number\":\n",
    "                value = prop_value.get(\"number\")\n",
    "            elif prop_type == \"url\":\n",
    "                value = prop_value.get(\"url\")\n",
    "            elif prop_type == \"date\":\n",
    "                value = prop_value.get(\"date\")\n",
    "            elif prop_type == \"select\":\n",
    "                value = prop_value.get(\"select\", {}).get(\"name\")\n",
    "            elif prop_type == \"multi_select\":\n",
    "                value = [item.get(\"name\") for item in prop_value.get(\"multi_select\", [])]\n",
    "            elif prop_type == \"checkbox\":\n",
    "                value = prop_value.get(\"checkbox\")\n",
    "            elif prop_type == \"email\":\n",
    "                value = prop_value.get(\"email\")\n",
    "            elif prop_type == \"phone_number\":\n",
    "                value = prop_value.get(\"phone_number\")\n",
    "            elif prop_type == \"people\":\n",
    "                value = [person.get(\"name\", \"\") for person in prop_value.get(\"people\", [])]\n",
    "            elif prop_type == \"files\":\n",
    "                value = [f.get(\"name\", \"\") for f in prop_value.get(\"files\", [])]\n",
    "            else:\n",
    "                value = f\"Unsupported type: {prop_type}\"\n",
    "\n",
    "            page_data[\"properties\"][prop_name] = value\n",
    "        \n",
    "        results.append(page_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_pages(num_pages=None):  \n",
    "    \"\"\"\n",
    "    If num_pages is None, get all pages, otherwise just the defined number.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.notion.com/v1/databases/{DATABASE_ID}/query\"\n",
    "\n",
    "    page_size = 1000 if num_pages==None else num_pages\n",
    "\n",
    "    payload = {\"page_size\": page_size}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    notion_data = response.json()\n",
    "\n",
    "    parsed_data = extract_notion_rows(notion_data)\n",
    "\n",
    "    # Comment this out to dump all data to a file\n",
    "    with open('notion_database.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(parsed_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d884fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9826879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1f088b77abf280608750fa0fd2366465'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.pop(\"NOTION_TOKEN\", None)\n",
    "os.environ.pop(\"DATABASE_ID\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "UPSTASH_REDIS_REST_URL = os.getenv(\"UPSTASH_REDIS_REST_URL\")\n",
    "UPSTASH_REDIS_REST_TOKEN = os.getenv(\"UPSTASH_REDIS_REST_TOKEN\")\n",
    "\n",
    "if not UPSTASH_REDIS_REST_URL or not UPSTASH_REDIS_REST_TOKEN:\n",
    "    raise ValueError(\"Missing Upstash credentials in environment\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {UPSTASH_REDIS_REST_TOKEN}\"\n",
    "}\n",
    "\n",
    "def list_upstash_keys(prefix=\"notion_\"):\n",
    "    \"\"\"Fetch all keys matching a prefix using Upstash Redis REST API.\"\"\"\n",
    "    url = f\"{UPSTASH_REDIS_REST_URL}\"  # No /keys route!\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {UPSTASH_REDIS_REST_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = [\"KEYS\", f\"{prefix}*\"]\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to list keys: {response.text}\")\n",
    "    \n",
    "    return response.json().get(\"result\", [])\n",
    "\n",
    "def get_upstash_json_by_key(key):\n",
    "    \"\"\"Get and parse JSON value for a specific key.\"\"\"\n",
    "    url = f\"{UPSTASH_REDIS_REST_URL}/get/{key}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch key {key}: {response.text}\")\n",
    "    raw_value = response.json().get(\"result\")\n",
    "    return json.loads(raw_value) if raw_value else []\n",
    "\n",
    "def load_dataset_from_upstash(prefix=\"notion_\"):\n",
    "    \"\"\"\n",
    "    Loads and combines documents from multiple JSON values stored in Upstash Redis.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    keys = list_upstash_keys(prefix=prefix)\n",
    "\n",
    "    print(keys[0])\n",
    "\n",
    "    for key in keys:\n",
    "        data = get_upstash_json_by_key(key)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(f\"Expected a list from key {key}, got {type(data)}\")\n",
    "\n",
    "        for entry in data:\n",
    "            text = json.dumps(entry['properties'], ensure_ascii=False, indent=2)\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"id\": entry.get(\"id\", \"\"),\n",
    "                    \"source_key\": key\n",
    "                }\n",
    "            )\n",
    "            all_documents.append(doc)\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks to improve retrieval performance.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        splits = text_splitter.split_text(doc.page_content)\n",
    "        for chunk in splits:\n",
    "            chunked_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "    return chunked_docs\n",
    "\n",
    "\n",
    "def create_vectorstore(documents):\n",
    "    \"\"\"\n",
    "    Creates a vectorstore by embedding document chunks using a local sentence-transformers model.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
